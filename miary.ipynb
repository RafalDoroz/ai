{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCCL2iyO/RJ8Ee8YfwU8Is",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RafalDoroz/ai/blob/main/miary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eo27LyjSyeZu",
        "outputId": "894ed15a-3e8d-4158-c2a4-3fcdc63d3ebf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Macierz Konfuzji ===\n",
            "True Negative (TN): 4 - Poprawne przewidywanie klasy negatywnej\n",
            "False Positive (FP): 1 - Fałszywe przewidywanie klasy pozytywnej\n",
            "False Negative (FN): 1 - Fałszywe przewidywanie klasy negatywnej\n",
            "True Positive (TP): 4 - Poprawne przewidywanie klasy pozytywnej\n",
            "\n",
            "=== Dokładność (Accuracy) ===\n",
            "Dokładność: 0.80\n",
            "Definicja: Procent poprawnych przewidywań.\n",
            "Wzór: (TP + TN) / (TP + TN + FP + FN) = (4 + 4) / 10\n",
            "\n",
            "=== Precyzja (Precision) ===\n",
            "Precyzja: 0.80\n",
            "Definicja: Procent przewidywanych pozytywnych, które są rzeczywiście pozytywne.\n",
            "Wzór: TP / (TP + FP) = 4 / (4 + 1)\n",
            "\n",
            "=== Czułość (Recall) ===\n",
            "Czułość: 0.80\n",
            "Definicja: Procent rzeczywistych pozytywnych, które zostały poprawnie wykryte.\n",
            "Wzór: TP / (TP + FN) = 4 / (4 + 1)\n",
            "\n",
            "=== F1-Score ===\n",
            "F1-Score: 0.80\n",
            "Definicja: Średnia harmoniczna precyzji i czułości.\n",
            "Wzór: 2 * (Precision * Recall) / (Precision + Recall)\n",
            "\n",
            "=== ROC AUC ===\n",
            "ROC AUC: 0.80\n",
            "Definicja: Miara zdolności modelu do rozróżniania klas (im bliżej 1, tym lepiej).\n",
            "\n",
            "=== Matthews Correlation Coefficient (MCC) ===\n",
            "MCC: 0.60\n",
            "Definicja: Miara korelacji pomiędzy przewidywaniami a rzeczywistymi klasami (od -1 do 1).\n",
            "\n",
            "=== Raport Klasyfikacji ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80         5\n",
            "           1       0.80      0.80      0.80         5\n",
            "\n",
            "    accuracy                           0.80        10\n",
            "   macro avg       0.80      0.80      0.80        10\n",
            "weighted avg       0.80      0.80      0.80        10\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, matthews_corrcoef, classification_report\n",
        "import numpy as np\n",
        "\n",
        "# 1. Prawdziwe etykiety (y_true) i przewidywane etykiety (y_pred)\n",
        "y_true = [1, 0, 1, 1, 0, 1, 0, 0, 1, 0]  # Rzeczywiste klasy\n",
        "y_pred = [1, 0, 1, 0, 0, 1, 0, 1, 1, 0]  # Przewidywane klasy\n",
        "\n",
        "# 2. Macierz konfuzji\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "tn, fp, fn, tp = cm.ravel()  # Rozpakowanie macierzy konfuzji\n",
        "\n",
        "print(\"=== Macierz Konfuzji ===\")\n",
        "print(f\"True Negative (TN): {tn} - Poprawne przewidywanie klasy negatywnej\")\n",
        "print(f\"False Positive (FP): {fp} - Fałszywe przewidywanie klasy pozytywnej\")\n",
        "print(f\"False Negative (FN): {fn} - Fałszywe przewidywanie klasy negatywnej\")\n",
        "print(f\"True Positive (TP): {tp} - Poprawne przewidywanie klasy pozytywnej\")\n",
        "\n",
        "# 3. Wyjaśnienie miar\n",
        "# (1) Dokładność\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(\"\\n=== Dokładność (Accuracy) ===\")\n",
        "print(f\"Dokładność: {accuracy:.2f}\")\n",
        "print(\"Definicja: Procent poprawnych przewidywań.\")\n",
        "print(f\"Wzór: (TP + TN) / (TP + TN + FP + FN) = ({tp} + {tn}) / {len(y_true)}\")\n",
        "\n",
        "# (2) Precyzja\n",
        "precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "print(\"\\n=== Precyzja (Precision) ===\")\n",
        "print(f\"Precyzja: {precision:.2f}\")\n",
        "print(\"Definicja: Procent przewidywanych pozytywnych, które są rzeczywiście pozytywne.\")\n",
        "print(f\"Wzór: TP / (TP + FP) = {tp} / ({tp} + {fp})\")\n",
        "\n",
        "# (3) Czułość\n",
        "recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "print(\"\\n=== Czułość (Recall) ===\")\n",
        "print(f\"Czułość: {recall:.2f}\")\n",
        "print(\"Definicja: Procent rzeczywistych pozytywnych, które zostały poprawnie wykryte.\")\n",
        "print(f\"Wzór: TP / (TP + FN) = {tp} / ({tp} + {fn})\")\n",
        "\n",
        "# (4) F1-Score\n",
        "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "print(\"\\n=== F1-Score ===\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(\"Definicja: Średnia harmoniczna precyzji i czułości.\")\n",
        "print(f\"Wzór: 2 * (Precision * Recall) / (Precision + Recall)\")\n",
        "\n",
        "# (5) ROC AUC\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "print(\"\\n=== ROC AUC ===\")\n",
        "print(f\"ROC AUC: {roc_auc:.2f}\")\n",
        "print(\"Definicja: Miara zdolności modelu do rozróżniania klas (im bliżej 1, tym lepiej).\")\n",
        "\n",
        "# (6) Matthews Correlation Coefficient (MCC)\n",
        "mcc = matthews_corrcoef(y_true, y_pred)\n",
        "print(\"\\n=== Matthews Correlation Coefficient (MCC) ===\")\n",
        "print(f\"MCC: {mcc:.2f}\")\n",
        "print(\"Definicja: Miara korelacji pomiędzy przewidywaniami a rzeczywistymi klasami (od -1 do 1).\")\n",
        "\n",
        "# 4. Pełny raport klasyfikacji\n",
        "print(\"\\n=== Raport Klasyfikacji ===\")\n",
        "print(classification_report(y_true, y_pred, zero_division=0))\n"
      ]
    }
  ]
}